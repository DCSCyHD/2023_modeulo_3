---
title: "Práctica 2 - Módulo 3"
subtitle: "Regresión lineal simple"
author: 
  - Diplomatura en Ciencias Sociales Computacionales y Humanidades Digitales (IDAES-UNSAM).
output: 
  html_document:
    toc: TRUE
    toc_float:
      toc_collapsed: true
    toc_depth: 3
    number_sections: true
    theme: readable
editor_options: 
  chunk_output_type: console
---

> Material elaborado originalmente por Carolina Pradier y Guido Weksler

```{r setup, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, tidy=FALSE,
                      fig.width=8)
```

```{r librerias, results='hide'}
library(tidyverse)
library(tidymodels)
library(GGally)
library(gtsummary)
library(gt)
tidymodels_prefer()

options(dplyr.summarise.inform = FALSE)
```

# Trabajando con tidymodels

El paquete *tidymodels* nos va a permitir construir modelos usando una lógica de sintaxis similar a la de *tidyverse*. Existen funciones análogas en el paquete *stats* que resultan menos intuitivas y claras para trabajar.

# Implementando una regresión lineal simple usando tidymodels

## Exploramos

Vamos a usar una base de **juguete** (es decir, con datos inventados), cuyas variables tienen nombres similares a los de la base de la Encuesta de Uso del Tiempo. Inventamos datos nuevos para que las relaciones sean claramente lineales (esto no suele pasar en ciencias sociales). Se busca entender la relación entre distintas características de uso del tiempo con el salario.

```{r}
base_juguete <- readRDS("./fuentes/eut_juguete.RDS")
```

```{r}
names(base_juguete)
```

```{r}
#modifico los nombres usando espacios para que entren mejor en las etiquetas
names(base_juguete) <- gsub(x = names(base_juguete),
                            pattern = "\\_",
                            replacement = " ")
names(base_juguete)
```

Podemos mirar una matriz descriptiva de la base usando *ggpairs()* para tener una idea de las relaciones entre las variables e ir pensando cuáles incorporamos a nuestro modelo.

```{r}
#graficamos
ggpairs(base_juguete, 
        labeller = label_wrap_gen(width = 5)) +  #Para que las etiquetas entren mejor
  theme_minimal() +
  theme(strip.text.x = element_text(size = 10),
        strip.text.y = element_text(size = 10))
```

Como siempre, podemos mejorar la visualización para enfocarnos en lo que más nos interese (en ese caso las diferencias de acuerdo con la variable sexo):

```{r}
ggpairs(base_juguete,
        ggplot2::aes(
          color = sexo,
          fill = sexo,
          alpha = 0.8
        ),
        labeller = label_wrap_gen(width = 5)) +
  theme_minimal() +
  scale_color_manual(values = c("#45818e", "#CA225E")) +
  scale_fill_manual(values = c("#45818e", "#CA225E")) +
  theme(strip.text.x = element_text(size = 10),
        strip.text.y = element_text(size = 10))
```

```{r include=F}
#vuelvo a dejar los nombres como estaban
base_juguete <- base_juguete %>% janitor::clean_names()
```

## Modelamos

Les proponemos usar el modelo de regresión lineal para estudiar la relación entre el ingreso individual de las personas (Y) y las horas que dedican por semana al trabajo doméstico (X). Es decir:

$$ IngInd = \beta_{0} + \beta_{1}HorasTD + \epsilon$$

------------------------------------------------------------------------

*Nota:* La correlación entre las horas de trabajo doméstico y el ingreso individual (ver abajo) no es necesariamente la más alta. ¿Por qué podría interesarnos enfocarnos en esta relación de todas formas?

```{r}
cor(base_juguete$horas_trabajo_domestico, 
    base_juguete$ingreso_individual)
```

------------------------------------------------------------------------

Veamos la relación gráficamente:

```{r}
base_juguete%>% 
  ggplot(aes(x=horas_trabajo_domestico, y=ingreso_individual)) + 
  geom_point() + 
  geom_smooth(
    method='lm', # Indica que queremos graficar una relación lineal
    se = FALSE, # Añadir el intervalo de confianza en torno a los puntos
    color = 'red'
  ) + 
  theme_minimal()
```

*geom_smooth()* nos ayuda a ver la relación lineal entre los puntos. Acá vemos que, si bien la relación entre ambas no es perfectamente lineal, la información sobre las horas dedicadas al trabajo doméstico ayuda a predecir sus ingresos individuales.

### tidymodels

Empecemos a implementar nuestro modelo.

Para empezar tenemos que indicarle a R:

1.  El **tipo** de modelo.

2.  El **modo**: regresión o clasificación (en este caso no es necesario aclararlo porque el tipo de modelo regresión lineal siempre es una regresión).

3.  El **motor** (el tipo de software que va a estar detrás del modelo).

```{r}
lm_model <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")

lm_model
```

Noten dos cosas importantes:

-   Esta misma estructura funciona para **cualquier modelo que quieran implementar**: tidymodels es muy flexible, sólo hay que modificar los parámetros de cada función. No es necesario aprender una serie de comandos para implementar KNN, otro para regresión logística, etc.

    ```         
    # tidymodels incluye al paquete parsnip, que provee una función para generar especificaciones de múltiples tipos de modelos.
    parsnip::parsnip_addin()
    ```

-   El momento de ingresar los datos y el momento de definir el modelo están **separados**. Podríamos trabajar con múltiples bases de datos aplicando siempre un modelo con estas especificaciones.

Podemos usar el comando *translate()* para que R nos indique qué le falta al modelo para realizar una estimación.

```{r}
lm_model %>% translate()
```

Una vez que especificamos cuál es nuestro modelo, vamos a estimarlo/"fitearlo" (es decir, estimar los coeficientes a partir de los datos del modelo).

```{r}
lm_fit <- lm_model %>%
  fit(ingreso_individual ~ horas_trabajo_domestico, 
      data = base_juguete)

#Siempre va "y" del lado izquierdo y "x" del lado derecho
```

Veamos los coeficientes:

```{r}
tabla_coeficientes <- lm_fit %>% tidy() 

tabla_coeficientes %>% 
  gt() %>%   #para visualizar de forma más prolija
  fmt_number(decimals=2) %>% 
  tab_header("Regresión lineal")
```

Noten que la información viene en forma de *data.frame*, por lo que es fácil extraer los coeficientes de la regresión.

```{r include=FALSE}
options(scipen = 999) #Evitar notación científica

intercepto <- round(tabla_coeficientes$estimate[1],1)
beta1 <- round(tabla_coeficientes$estimate[2])
```

De acuerdo con nuestra estimación:

$\hat{IngInd} =$ `r intercepto` $+$ `r beta1` $HorasTD$


```{r include=FALSE}
options(scipen = 0) #notación científica
```

Si queremos más información sobre nuestro modelo, podemos mirar qué hay dentro del objeto `lm_fit` (una lista). A su vez, dentro del elemento "fit" encontramos el $R^2$ y otra información. 

```{r}
names(lm_fit)

lm_fit %>% 
  pluck("fit") %>% #Pluck es la versión "tidy" de lm_fit[["fit"]]
  summary()
```

¿Qué información se puede leer? ¿Las relaciones son estadísticamente significativas? ¿El modelo es "bueno"?

------------------------------------------------------------------------

**Ejercicio**

Plantee un modelo donde la variable explicativa sea la variable horas_trabajo_mercado, compare con el modelo anterior.

```{r}

```

```{r}
#respuesta
lm_model %>%
  fit(ingreso_individual ~ horas_trabajo_mercado, 
      data = base_juguete) %>%
  pluck("fit") %>%
  summary()
```

------------------------------------------------------------------------

## Predecimos

Para que el modelo nos arroje sus predicciones, usamos el comando *predict()*, donde tenemos que especificar el modelo y los datos sobre los cuales predecir. Como en este caso queremos recuperar las predicciones "dentro de la muestra", usamos los mismos datos.

Podemos ver tanto la predicción puntual para cada observación (por default), como un intervalo de confianza.

```{r}
lm_fit %>% 
  predict(new_data = base_juguete
          #,type = "pred_int"  #Si queremos el intervalo de confianza de la prediccion
          )
```

------------------------------------------------------------------------

*Nota:* por ahora no estamos usando sets de train y test, por lo que predecimos sobre los mismos datos sobre los que entrenamos el modelo. Cuando empecemos a usar train y test, vamos a ingresar el set de test en el argumento *new_data*.

------------------------------------------------------------------------

También podemos usar la función *augment()* para anexar nuestras predicciones y sus residuos a la base. Es lo mismo que *predict()* pero nos pega todos los datos.

```{r}
lm_fit %>% 
  augment(new_data = base_juguete)
```

Grafiquemos los residuos para ver si hay algún problema con el modelo.

```{r}
lm_fit %>% 
  augment(new_data = base_juguete) %>%
  dplyr::select(.resid, .pred) %>%
  ggplot(aes(x = .pred, y = .resid)) +
  geom_point() +
  theme_minimal() +
  geom_hline(yintercept = 0, linetype = 'dashed') 
```

Si queremos inspeccionar, podemos usar pull() (que es como $) para acceder a la columna de residuos y calcular el máximo.

```{r}
lm_fit %>% 
  augment(new_data = base_juguete) %>% 
  pull(.pred) %>% 
  max()

lm_fit %>% 
  augment(new_data = base_juguete) %>% 
  pull(ingreso_individual) %>% 
  max()

```

¿Dónde funciona mal el modelo? Para los casos con ingresos altos y muy pocas horas de trabajo no remunerado.

Quizás falta agregar alguna variable al modelo

```{r}
base_juguete %>%
  ggplot(aes(x = horas_trabajo_domestico, 
             y = ingreso_individual, 
             color = sexo)) +
  geom_point() +
  geom_smooth(method = 'lm', color = 'black', se = FALSE) +
  theme_minimal()
```

Vemos que la relación entre ambas variables es muy distinta para mujeres y varones. En la próxima clase vamos a probar incorporar más variables a nuestra regresión para mejorar nuestros resultados.

# Resumen

Para implementar una regresión lineal simple en tidymodels necesitamos:

1.  Especificar el modelo. Por ejemplo lm_model <- linear_reg() (aclarar mode y engine)

2.  Estimarlo con fit() (aclarando con qué datos)

3.  Usarlo, por ejemplo para recuperar las predicciones con predict() o augment()

# Referencias bibliográficas

- An Introduction to Statistical Learning with applications in R (James, Witten, Hastie y Tibshirani) -- 1st and 2nd version

- Tidy Modeling with R (Kuhn y Silge)

- Introduction to Modern Statistics (Çetinkaya-Rundel y Hardin)
