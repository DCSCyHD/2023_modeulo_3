---
title: "Práctica 6 - Módulo 3"
subtitle: "Cross-Validation"
author: 
  - Diplomatura en Ciencias Sociales Computacionales y Humanidades Digitales (IDAES-UNSAM)
output: 
  html_document:
    toc: TRUE
    toc_float:
      toc_collapsed: true
    toc_depth: 3
    number_sections: true
    theme: readable
editor_options: 
  chunk_output_type: console
---

> Material elaborado originalmente por Carolina Pradier y Guido Weksler

```{r setup, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, tidy=FALSE,
                      fig.width=8)
```

```{r librerias, results='hide'}
library(tidyverse)
library(tidymodels)
library(kknn)
library(gt)
library(discrim)

options(dplyr.summarise.inform = FALSE, scipen = 999)
theme_set(theme_bw())
```

# Introducción

En este material aplicaremos la técnica de **cross-validation** para testear distintos modelos vistos a lo largo del módulo. Utilizaremos las herramientas específicas de tidymodels para aprender a comparar modelos.

# Cross-Validation

La función `vfold_cv()` crea un objeto que contiene las particiones. Usaremos **v = 10**, lo que implica que se particionará el dataset en 10 (folds). En cada una de las 10 iteraciones se toman 9 particiones para entrenamiento o ajuste del modelo y 1 partición para la evaluación. Esto permite obtener 10 métricas de performance del modelo, después las promediamos y elegimos el mejor modelo en base a ese promedio (también podemos querer mirar el desvío estándar para evaluar cuán representativo es ese promedio).

Normalmente primero separaríamos una muestra de test, para hacer una evaluación "vf final final", y aplicaríamos CV solo sobre la muestra de train. Pero dejémoslo de lado para concentrarnos en cómo funciona CV.

```{r}
base_juguete <- readRDS("fuentes/eut_juguete.RDS")

set.seed(42)
base_folds <- vfold_cv(data = base_juguete, v = 10)

base_folds

```

Comencemos con un caso sencillo, un modelo de regresión lineal con dos predictores.

```{r}
# Especificación del modelo
lm_model <- linear_reg() %>%
  set_engine("lm") %>% 
  set_mode("regression")

# Receta/fórmula 
lm_rec <- recipe(formula = horas_trabajo_domestico ~ horas_trabajo_mercado +
                                                     ingreso_individual,
                 data =  base_juguete)
```

Se ajusta el modelo en validación cruzada, obteniendo las métricas de interés para cada uno de los folds

-   RMSE

-   MAE

```{r}
lm_val_fit <- fit_resamples(
  object       = lm_model, # Definición de mi (mis) modelos
  preprocessor = lm_rec, # Fórmula a aplicar
  resamples    = base_folds, # De donde saco las particiones
  metrics = metric_set(rmse, mae), # Root mean squear error y Mean abs error
  control = control_resamples(save_pred = TRUE) # Guardar predicciones
)
```

¿Que contiene **lm_valfit**? Para cada fold (que se corresponde con una iteración distinta) contiene las métricas y las predicciones. 

```{r}
head(lm_val_fit)
```

Si queremos extraer las predicciones, le pedimos la columna de predicciones. Como cada columna es una lista, le pedimos el primer elemento (el único que hay), que es el data.frame con las predicciones.

```{r}
lm_val_fit %>% 
  pull(.predictions) %>% 
  pluck(1)
```


Con la función collect_metrics, puedo hacer un resumen de todas las métricas obtenidas en cada uno de mis folds. Es decir, promedio el MAE (mean absolute error) obtenido con cada fold y también promedio el RMSE (root mean squeared error) obtenido con cada fold.

```{r}
lm_val_fit %>% 
  collect_metrics()
```


# Comparando modelos

Ahora bien, todo esto fue aplicado solamente con un modelo (regresión lineal múltiple de grado 1). La gracia es comparar múltiples modelos, para elegir el mejor.

Vamos a crear ahora una receta básica, para luego agregarle o quitarle cosas a esa receta y comparar cómo performa una regresión lineal múltiple con las distintas recetas. Así podríamos elegir el mejor modelo de regresión lineal.

```{r}
receta_basica <- recipe(
  horas_trabajo_domestico ~ horas_trabajo_mercado + 
                            ingreso_individual + 
                            sexo +
                            menores_hogar,
  data = base_juguete
)

receta_2 <- receta_basica  %>%
    # Agrego término al cuadrado para variables numericas
    step_poly(all_numeric_predictors(), 
              degree = 2, 
              options = c(raw = TRUE)) 

receta_3 <- receta_basica  %>%
    # Saco 2 variables a ver qué pasa
    step_rm(c(menores_hogar,ingreso_individual))

recetario <- list(
  basica = receta_basica,
  poly = receta_2,
  dos_x = receta_3
)

modelos <- list(
  reg_lineal = lm_model
)
```

De la combinación de las distintas recetas ("preprocesadores") y modelos (en este caso pusimos solo regresión lineal) surgen distintos modelos o workflows. Para trabajar con todos a la vez, podemos incluirlos dentro de un **workflowset**. 

```{r}
lm_wflowset <- workflow_set(
  preproc = recetario, # Recetas a aplicar
  models = modelos,    # Lista de modelos (en este caso es uno solo)
  cross = TRUE)        #¿quiero hacer todas las combinaciones de recetas-modelos?

lm_wflowset
```

Hasta acá tengo el listado de todos mis workflows (combinación de recetas y modelos), pero no realicé ningún tipo de entrenamiento. La función \``workflow_map` permite aplicar un mismo procedimiento a todos los modelos. En nuestro caso, con "fit_resamples" aplicamos CV.

```{r}
lm_wflowset_fit <- lm_wflowset %>% 
  workflow_map(fn = "fit_resamples",   #Hacer cross validation
               resamples = base_folds, #Con que folds
               metrics = metric_set(rmse, mae), #Recolectando cuales metricas
               seed = 1101            #Semilla para reproducibilidad
               ) 
```

Recolecto métricas del CV (el promedio de los 10 folds), de los 3 modelos al mismo tiempo. Puedo comparar muy fácilmente qué modelo performa mejor en términos de la métrica elegida.

```{r}
collect_metrics(lm_wflowset_fit)
```

Armemos una tabla un poco más linda.

```{r}
collect_metrics(lm_wflowset_fit) %>%
  select(wflow_id, .metric, mean) %>%
  pivot_wider(id_cols = wflow_id, names_from=.metric, values_from = mean) %>%
  gt() %>% fmt_number()
```

El mejor modelo parece ser el que incluye los términos polinómicos, porque tiene un menor error de prediccion fuera de la muestra (ya sea si medimos el error absoluto o la raíz del error cuadrático medio).

# Tuneando hiperparametros

## Regresión

El cross-validation se usa para comparar las predicciones fuera de la base de entrenamiento, y con eso determinar cuál es el mejor modelo. Recien lo aplicamos a distintas recetas de un mismo tipo de modelo (regresión lineal), pero también podríamos usarlo para:

1. Distintos tipos de modelos con la misma receta.
2. Distintos tipos de modelos con distintas recetas.
3. Un mismo tipo de modelo, con distintos hiperparámetros.

Para los primeros dos casos, alcanza con ajustar el workflowset. El último es lo que se conoce como *tuneo* ("espanglish" de tune: afinación). Consiste en evaluar cual es el mejor valor posible para un hiperparámetro en función de la capacidad de predecir fuera de la muestra.

Hagamos otro ejercicio de predicción un poco distinto. Queremos predecir el **ingreso individual** a partir de las horas de trabajo, el sexo y la cantidad de menores en el hogar. Tenemos la intuición de que **los predictores numéricos no se relacionan linealmente con la variable objetivo**. ¿Cómo hacemos para saber hasta que grado de polinomio nos conviene avanzar? El **grado** del polinomio es un hiperparámetro.

Primero definimos la receta básica.

```{r}
receta_basica <- recipe(
  ingreso_individual ~ horas_trabajo_mercado +
                       sexo +
                       menores_hogar,
  data = base_juguete)
```

Ahora agregamos un step con los polinomios, pero en vez de fijar el grado, especificamos que lo vamos a tunear con `tune()`. 

```{r}
receta_para_tunear <- receta_basica %>%
  step_poly(all_numeric_predictors(), 
            degree = tune()             #ACA LA CLAVE
  )

#Creamos el workflow y agregamos la receta y el modelo
workflow_tuneo <- workflow() %>%
  add_recipe(receta_para_tunear) %>% 
  add_model(lm_model)

```

Por otro lado tenemos que definir los valores de los parámetros que queremos comparar. Para eso armamos una grilla (una tibble) donde cada columna es un parámetro distinto, y el contenido son los posibles valores. Es importante que el nombre de la columna de la grilla coincida con el nombre del parámetro al que le pusimos `tune()`.

```{r}
grilla <- tibble(degree = 1:5)
```

Ahora con la función `tune_grid()` aplicamos v-fold CV para calcular las métricas de todas las posibles combinaciones de parámetros en la grilla.

```{r}
set.seed(42)

tune_res <- tune_grid(
  object = workflow_tuneo,   # Qué modelo voy a tunear
  resamples = base_folds,    # De dónde saco los folds de datos 
  grid = grilla,             # Hiperparametros a evaluar
  metrics = metric_set(rmse, rsq) # Métricas a evaluar: RMSE y R^2 ajustado
)
```

La función autopolot() genera un plot automático a partir del resultado del tuneo (tune_res). Nos muestra la métrica promedio obtenida del CV. 

```{r}
tune_res %>% autoplot()
```

Vemos que polinomios de grado 4 parecen devolver las mejores predicciones. 

Si estuviera trabajando con muchísimos valores posibles a evaluar, la función `show_best()` me permite especificar cual es mi métrica de interés y me devuelve los *n* mejores valores de hiperparámetros en ese sentido.

```{r}
tune_res %>% 
  show_best(metric = "rmse", n = 2)
```

## Elegir entre múltiples modelos tuneados

Queremos elegir entre múltiples modelos de clasificación para la variable "realiza_trabajo_domestico". Definimos los folds, aplicando estratos para evitar que algunos tengan muy pocas observaciones que no realicen trabajo doméstico (como habíamos hecho al balancear la muestra).

```{r}
set.seed(42)
folds <- vfold_cv(data = base_juguete, 
                  v = 5, 
                  strata='realiza_trabajo_domestico')

```

Definimos 2 recetas:

```{r}
receta_basica <- recipe(
  formula = realiza_trabajo_domestico ~ horas_trabajo_mercado + 
                                        menores_hogar + 
                                        sexo + 
                                        ingreso_individual,
  data = base_juguete) %>% 
  # Codificamos variable sexo
  step_dummy(all_nominal_predictors()) 

# Extendemos
receta_extendida <- receta_basica %>% 
  
  #Interacciones
  step_interact(~ all_predictors():all_predictors()) %>% 
  
  #Elimino variables altamente correlacionadas
  step_corr(all_numeric_predictors(), 
            threshold = 0.8) %>% 
  
  #Elimino variables con varianza cercana a 0
  step_nzv(all_predictors()) 
```

Definimos también 2 modelos: un KNN cuyo K vamos a tunear, y un QDA.

```{r}
# KNN 
knn_model <- nearest_neighbor(
    neighbors = tune(),
    weight_func = "rectangular") %>% 
  set_engine("kknn") %>% 
  set_mode("classification")

# QDA
qda_model <- discrim_quad() %>% 
  set_engine("MASS") %>% 
  set_mode("classification")

```

Ahora armamos nuestro workflowset, que va a ser todas las combinaciones posibles entre las dos recetas y los dos modelos. Si fijáramos `cross=FALSE` nos calcularía el primer modelo con la primera receta, el segundo con la segunda, etc.

```{r}
wflowset <- workflow_set(
      preproc = list(basica = receta_basica, 
                     extendida = receta_extendida),
      models = list(knn = knn_model,
                    qda = qda_model),
      cross = TRUE #Todas las combinaciones posibles 
   )
```

Realizamos el ajuste de hiperparámetros. Primero definimos la grilla y luego aplicamos CV. 

```{r}
grilla <- c(1,4,6)

set.seed(42)

wflowset_fit <- wflowset %>%
  workflow_map(
    fn = "tune_grid",
    resamples = folds,
    grid = grilla,
    metrics = metric_set(accuracy, roc_auc))
```

Se visualizan las métricas (automáticamente me arma un rank según la primera métrica). Automáticamente va a calcular las métricas para cada workflow eligiendo el mejor hiper parámetro.

```{r}
wflowset_fit %>% autoplot()
```

Para saber qué combinación modelo-receta es la mejor puedo rankear los resultados en una tabla.

```{r}
wflowset_fit %>% rank_results(rank_metric = "accuracy",
                               select_best = TRUE) %>%
  dplyr::select(wflow_id, .metric, mean) %>%
  pivot_wider(id_cols = wflow_id,
              names_from = .metric,
              values_from = mean) 
```

¿Cuáles fueron los hiperparámetros que mejor funcionaron para un workflow específico?

```{r}
# Se extrae el workflow
wflowset_fit %>% 
  extract_workflow_set_result(id = 'extendida_knn') %>% 
  collect_metrics()
```



